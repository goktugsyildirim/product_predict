{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54d136a8-ac90-4eba-b2d1-e1cfad03d4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/gok2s/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/gok2s/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import time\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07295bdc-5ae7-45e2-8f72-da45b27f7c77",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing Functions\n",
    "Data sets had to go through a process. Because some tuples contained more than one semicolon, which made it difficult to extract the data. Some tuples had a \"null\" value or no value at all. These also had to be resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2c05090-5507-4f9e-92bd-9514bf508e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Loading and Preprocessing Functions\n",
    "def load_data():\n",
    "    def read_file(file_path):\n",
    "        data = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                split_line = line.split(';', 1)  # Split only at the first semicolon\n",
    "                if len(split_line) == 2:\n",
    "                    data.append(split_line)\n",
    "        return data\n",
    "\n",
    "    categories = read_file('data/Product_Categories.txt')\n",
    "    explanations = read_file('data/Product_Explanation.txt')\n",
    "\n",
    "    categories_df = pd.DataFrame(categories, columns=['Product_ID', 'Category'])\n",
    "    explanations_df = pd.DataFrame(explanations, columns=['Product_ID', 'Description'])\n",
    "\n",
    "    data = pd.merge(explanations_df, categories_df, on='Product_ID')\n",
    "    data.dropna(subset=['Category', 'Description'], inplace=True)  # Drop rows with null values in either Category or Description\n",
    "    return data\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation + string.digits))  # Remove punctuation and digits\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('turkish')]  # Remove stop words\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_data(data):\n",
    "    data['Cleaned_Description'] = data['Description'].apply(clean_text)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(data['Cleaned_Description'])\n",
    "    y = data['Category']\n",
    "    return X, y, vectorizer\n",
    "\n",
    "def save_preprocessed_data(X, y, vectorizer, file_prefix='preprocessed'):\n",
    "    with open(f'{file_prefix}_X.pkl', 'wb') as f:\n",
    "        pickle.dump(X, f)\n",
    "    with open(f'{file_prefix}_y.pkl', 'wb') as f:\n",
    "        pickle.dump(y, f)\n",
    "    with open(f'{file_prefix}_vectorizer.pkl', 'wb') as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "\n",
    "def load_preprocessed_data(file_prefix='preprocessed'):\n",
    "    with open(f'{file_prefix}_X.pkl', 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    with open(f'{file_prefix}_y.pkl', 'rb') as f:\n",
    "        y = pickle.load(f)\n",
    "    with open(f'{file_prefix}_vectorizer.pkl', 'rb') as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "    return X, y, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "488830ad-4e1a-4825-9a92-8264b549f674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data...\n",
      "Data loaded and preprocessed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Data Loading and Preprocessing\n",
    "# Check if preprocessed data exists\n",
    "# While testing designed for efficiency if you want you can delete files to execute \"else\" part\n",
    "if os.path.exists('preprocessed_X.pkl') and os.path.exists('preprocessed_y.pkl') and os.path.exists('preprocessed_vectorizer.pkl'):\n",
    "    print(\"Loading preprocessed data...\")\n",
    "    X, y, vectorizer = load_preprocessed_data()\n",
    "else:\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    data = load_data()\n",
    "    X, y, vectorizer = preprocess_data(data)\n",
    "    save_preprocessed_data(X, y, vectorizer)\n",
    "print(\"Data loaded and preprocessed successfully.\")\n",
    "\n",
    "# Scaling the data\n",
    "# Scaling the data ensures that all features are on a similar scale, which helps the models to learn more effectively and efficiently. \n",
    "# This is especially important when using algorithms sensitive to feature scaling.\n",
    "scaler = StandardScaler(with_mean=False)  # with_mean=False to avoid issues with sparse matrices\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b2a989-5ef6-4724-94ff-7a35eaca394b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before balancing:\n",
      "Category\n",
      "\\n                                                              29\n",
      "Bilgisayar Ürünleri > Tüketim Malzemeleri > Kartuş\\n            12\n",
      "Telefon > Cep Telefonu\\n                                        11\n",
      "Toner Kartuş Şerit > Kartuş\\n                                   11\n",
      "Yazılım > Oyunlar > PC > ARAL\\n                                 10\n",
      "                                                                ..\n",
      "BİLGİSAYAR > AĞ/MODEM > MODEM\\n                                  1\n",
      "OEM Ürünleri > Soğutma Sistemleri > İşlemci > ZALMAN\\n           1\n",
      "Fotoğraf Makinesi Aksesuarı > Addison\\n                          1\n",
      "OEM Ürünleri > Soğutma Sistemleri > Sıvı Soğutma > CORSAIR\\n     1\n",
      "Corsair Force GS 240 GB SSD Disk CSSD-F240GBGS-BK\\n              1\n",
      "Name: count, Length: 1165, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Handling Class Imbalance with SMOTE\n",
    "# Examine the distribution of classes\n",
    "class_distribution = pd.Series(y).value_counts()\n",
    "print(\"Class distribution before balancing:\")\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3beb66-b79d-457a-96d1-f35e2a3e2cf8",
   "metadata": {},
   "source": [
    "I am removing classes each containing less than 5 examples in order to avoid memorizing these less frequent classes by a model and to make the dataset more balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2515015c-c8aa-4415-b8f5-d7990da25244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare classes\n",
    "min_samples = 5  # Minimum number of samples required for each class\n",
    "filtered_classes = class_distribution[class_distribution >= min_samples].index\n",
    "X = X[y.isin(filtered_classes)]\n",
    "y = y[y.isin(filtered_classes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcad242-f559-4ec8-8988-3a29c964f9be",
   "metadata": {},
   "source": [
    "To create artificial sample sizes for the minority segments, SMOTE is used. The result of this would be an improvement of model performance as well as generalization while at the same time ensuring that each training instance sufficiently represents every category through attaining balance in the distribution of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9ed93e4-fbfa-4fa8-823b-24810b39b6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling class imbalance using SMOTE...\n",
      "Class distribution after balancing:\n",
      "Category\n",
      "Bilgisayar > Taşınabilir Bilgisayarlar\\n                                             29\n",
      "Bilgisayar Ürünleri > Tüketim Malzemeleri > Kartuş\\n                                 29\n",
      "Kamera\\n                                                                             29\n",
      "Elektronik Televizyon > Televizyon > LED Televizyon\\n                                29\n",
      "Bilgisayar Bileşenleri > Ses Kartları > ASUS\\n                                       29\n",
      "Bilgisayar Bileşenleri > Bellekler > CORSAIR\\n                                       29\n",
      "Bilgisayar > Yazıcılar > Kartuş-Toner-Drum\\n                                         29\n",
      "Aksesuar Ürünleri > Kablolar > Görüntü Kabloları > S-LINK\\n                          29\n",
      "OYUN &AMP HOBİ > OYUN & YAZILIM > PC\\n                                               29\n",
      "Oyun ve Oyun Konsolu > Oyunlar\\n                                                     29\n",
      "Tüketim Malzemeleri > Kartuş > HP\\n                                                  29\n",
      "BİLGİSAYAR > SARF MALZEMELERİ > TONER & KARTUŞ\\n                                     29\n",
      "Bilgisayar > Ofis Malzemeleri > Kartuşlar\\n                                          29\n",
      "Elektronik > Televizyon > TV Aksesuarları > Kablo - Sarf Malzemeler\\n                29\n",
      "Bilgisayar > Çevre Birimleri > Monitörler\\n                                          29\n",
      "Bilgisayar > Sarf Malzemeler > Mürekkep - Kartuş\\n                                   29\n",
      "Bilgisayar > Yazıcı & Sarf Malzemeleri > Sarf Malzemeleri > Mürekkep Kartuşları\\n    29\n",
      "\\n                                                                                   29\n",
      "Bilgisayar > Bilgisayar Aksesuarları > Kablolar\\n                                    29\n",
      "Tüketim Ürünleri > Sarf Malzemeler > Toner\\n                                         29\n",
      "Oyun - Hobi > Oyun > Oyunlar\\n                                                       29\n",
      "Telefon > Cep Telefonları\\n                                                          29\n",
      "Bilgisayar > Çevre Birimleri > Kulaklık\\n                                            29\n",
      "Ev Elektroniği > Televizyonlar\\n                                                     29\n",
      "Yazılım > Oyunlar > PC > ARAL\\n                                                      29\n",
      "Telefon > Cep Telefonu\\n                                                             29\n",
      "Antenler / Kablolar > S-Link\\n                                                       29\n",
      "Bilgisayar > Çevre Birimleri > Mouse\\n                                               29\n",
      "Bilgisayar > Tablet\\n                                                                29\n",
      "Bilgisayar > Yazılım Ürünleri > Oyunlar > Bilgisayar Oyunları\\n                      29\n",
      "Bilgisayar > Veri Depolama\\n                                                         29\n",
      "Telefon > Akıllı Telefonlar\\n                                                        29\n",
      "Toner Kartuş Şerit > Kartuş\\n                                                        29\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Handling class imbalance using SMOTE...\")\n",
    "smote = SMOTE(k_neighbors=1, random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "print(\"Class distribution after balancing:\")\n",
    "print(pd.Series(y_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56b35988-3da0-41f2-9fd8-ecf495c35241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into training and testing sets...\n",
      "Data split successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Splitting Data into Training and Testing Sets\n",
    "print(\"Splitting data into training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "print(\"Data split successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6df19b2e-9d53-40bd-9a8b-313b5d62f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Helper Function to Print Classification Report\n",
    "def print_classification_report(report):\n",
    "    print(\"accuracy: \", report['accuracy'])\n",
    "    print(\"macro avg: \", report['macro avg'])\n",
    "    print(\"weighted avg: \", report['weighted avg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abe079d-5bd3-4d9d-9735-720452fa9f3a",
   "metadata": {},
   "source": [
    "# Approach\n",
    "I used many different types of machine learning models such as Logistic Regression, Support Vector Machine (SVM), Random Forest, Decision Tree, Naive Bayes, and K-Nearest Neighbors (KNN) models to forecast product categories grounded in descriptions. I prepared the dataset after balancing class distribution with SMOTE and scaling the features with StandardScaler. I performed hyperparameter tuning using GridSearchCV for each model to optimize performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf336fc-5ff3-4e8c-8791-454f11a87cbc",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b06facb-3927-479a-a1c3-0edcf4e1df01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "accuracy:  0.8072916666666666\n",
      "macro avg:  {'precision': 0.8153679653679653, 'recall': 0.8098965848965849, 'f1-score': 0.7793150150129555, 'support': 192.0}\n",
      "weighted avg:  {'precision': 0.8525669642857142, 'recall': 0.8072916666666666, 'f1-score': 0.7981469344813167, 'support': 192.0}\n",
      "Best Score for Logistic Regression:  0.8013071895424837\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Logistic Regression...\")\n",
    "logreg = LogisticRegression(class_weight='balanced', max_iter=2000, solver='liblinear', tol=1e-3)\n",
    "param_grid_logreg = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "grid_logreg = GridSearchCV(logreg, param_grid_logreg, cv=5)\n",
    "grid_logreg.fit(X_train, y_train)\n",
    "y_pred_logreg = grid_logreg.predict(X_test)\n",
    "report_logreg = classification_report(y_test, y_pred_logreg, output_dict=True, zero_division=0)\n",
    "print_classification_report(report_logreg)\n",
    "print(\"Best Score for Logistic Regression: \", grid_logreg.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0004d15-dd15-4238-bc4a-2efcd8deaa1b",
   "metadata": {},
   "source": [
    "Balanced performance with a good precision-recall balance, effective for interpretable models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08645a07-00b4-4d8f-bb34-bdddab711a5e",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "246d6f39-d982-48cb-8cdb-674cea67fe89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Support Vector Machine...\n",
      "accuracy:  0.8125\n",
      "macro avg:  {'precision': 0.8257798646814666, 'recall': 0.8098965848965849, 'f1-score': 0.7858900358900359, 'support': 192.0}\n",
      "weighted avg:  {'precision': 0.8638018279394138, 'recall': 0.8125, 'f1-score': 0.8063647868335369, 'support': 192.0}\n",
      "Best Score for Support Vector Machine:  0.7830065359477123\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Support Vector Machine...\")\n",
    "svm = SVC(class_weight='balanced')\n",
    "param_grid_svm = {'C': [0.01, 0.1, 1, 10, 100], 'kernel': ['linear', 'rbf']}\n",
    "grid_svm = GridSearchCV(svm, param_grid_svm, cv=5)\n",
    "grid_svm.fit(X_train, y_train)\n",
    "y_pred_svm = grid_svm.predict(X_test)\n",
    "report_svm = classification_report(y_test, y_pred_svm, output_dict=True, zero_division=0)\n",
    "print_classification_report(report_svm)\n",
    "print(\"Best Score for Support Vector Machine: \", grid_svm.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d497f6a9-8b38-4110-807b-991d1d65c254",
   "metadata": {},
   "source": [
    "Best individual performer in terms of accuracy and precision, handling the complexity of the dataset well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c6a04d-1dc8-41a1-9d0d-3398e8d823d2",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec884a3c-5719-443e-a9d1-f6f50935d63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest...\n",
      "accuracy:  0.8072916666666666\n",
      "macro avg:  {'precision': 0.813746487430698, 'recall': 0.8048460798460798, 'f1-score': 0.777829383092541, 'support': 192.0}\n",
      "weighted avg:  {'precision': 0.8488917606516292, 'recall': 0.8072916666666666, 'f1-score': 0.7965859828524303, 'support': 192.0}\n",
      "Best Score for Random Forest:  0.7908496732026145\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Random Forest...\")\n",
    "rf = RandomForestClassifier(class_weight='balanced')\n",
    "param_grid_rf = {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, 20]}\n",
    "grid_rf = GridSearchCV(rf, param_grid_rf, cv=5)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "y_pred_rf = grid_rf.predict(X_test)\n",
    "report_rf = classification_report(y_test, y_pred_rf, output_dict=True, zero_division=0)\n",
    "print_classification_report(report_rf)\n",
    "print(\"Best Score for Random Forest: \", grid_rf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17edf18-efb9-4dce-a892-f08262b0dea5",
   "metadata": {},
   "source": [
    "Robust performance, useful for datasets with varied feature importance, though slightly less accurate than SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f3f754-5551-4ea3-b698-078a4908f86a",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88245a1f-8ccf-4f9f-868a-d9193a26df59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree...\n",
      "accuracy:  0.8072916666666666\n",
      "macro avg:  {'precision': 0.8068421052631579, 'recall': 0.7823713323713324, 'f1-score': 0.7575994829566848, 'support': 192.0}\n",
      "weighted avg:  {'precision': 0.860076754385965, 'recall': 0.8072916666666666, 'f1-score': 0.7982516120827144, 'support': 192.0}\n",
      "Best Score for Decision Tree:  0.7450980392156863\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Decision Tree...\")\n",
    "dt = DecisionTreeClassifier(class_weight='balanced')\n",
    "param_grid_dt = {'max_depth': [5, 10, 20, 30]}\n",
    "grid_dt = GridSearchCV(dt, param_grid_dt, cv=5)\n",
    "grid_dt.fit(X_train, y_train)\n",
    "y_pred_dt = grid_dt.predict(X_test)\n",
    "report_dt = classification_report(y_test, y_pred_dt, output_dict=True, zero_division=0)\n",
    "print_classification_report(report_dt)\n",
    "print(\"Best Score for Decision Tree: \", grid_dt.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4993af-b671-4db0-a859-c1bd49861297",
   "metadata": {},
   "source": [
    "Comparable to Logistic Regression, simpler to interpret but can overfit without proper tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c775350-dcb4-47bd-9ec5-f2b8c38f5305",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fe20ac7-5f2d-49a0-a27b-920833697e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Naive Bayes...\n",
      "accuracy:  0.7760416666666666\n",
      "macro avg:  {'precision': 0.7981240981240981, 'recall': 0.7678571428571428, 'f1-score': 0.7443008201522134, 'support': 192.0}\n",
      "weighted avg:  {'precision': 0.8564980158730159, 'recall': 0.7760416666666666, 'f1-score': 0.7854802075003313, 'support': 192.0}\n",
      "Best Score for Naive Bayes:  0.7660130718954248\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Naive Bayes...\")\n",
    "nb = MultinomialNB()\n",
    "param_grid_nb = {'alpha': [0.01, 0.1, 1, 10]}\n",
    "grid_nb = GridSearchCV(nb, param_grid_nb, cv=5)\n",
    "grid_nb.fit(X_train, y_train)\n",
    "y_pred_nb = grid_nb.predict(X_test)\n",
    "report_nb = classification_report(y_test, y_pred_nb, output_dict=True, zero_division=0)\n",
    "print_classification_report(report_nb)\n",
    "print(\"Best Score for Naive Bayes: \", grid_nb.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393a164c-152c-46f1-b480-40bf94dd45c8",
   "metadata": {},
   "source": [
    "Consistent but generally lower performance, suited for simpler, more linear problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66b752f-4553-4d4f-825a-89272da6be12",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5c735fc-3da9-4cb3-b654-55cd87bc31d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training K-Nearest Neighbors...\n",
      "accuracy:  0.8229166666666666\n",
      "macro avg:  {'precision': 0.8318834275772075, 'recall': 0.8136844636844637, 'f1-score': 0.7927287527287528, 'support': 192.0}\n",
      "weighted avg:  {'precision': 0.8713741028708134, 'recall': 0.8229166666666666, 'f1-score': 0.8162367724867724, 'support': 192.0}\n",
      "Best Score for K-Nearest Neighbors:  0.7725490196078431\n"
     ]
    }
   ],
   "source": [
    "print(\"Training K-Nearest Neighbors...\")\n",
    "knn = KNeighborsClassifier()\n",
    "param_grid_knn = {'n_neighbors': [1, 3, 5, 7, 9]}\n",
    "grid_knn = GridSearchCV(knn, param_grid_knn, cv=5)\n",
    "grid_knn.fit(X_train, y_train)\n",
    "y_pred_knn = grid_knn.predict(X_test)\n",
    "report_knn = classification_report(y_test, y_pred_knn, output_dict=True, zero_division=0)\n",
    "print_classification_report(report_knn)\n",
    "print(\"Best Score for K-Nearest Neighbors: \", grid_knn.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10250d3-8d1a-40b0-8f5f-4e502ca654b4",
   "metadata": {},
   "source": [
    "High accuracy, effective for capturing local structure in the data, but computationally intensive for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b67f848-338f-4bd3-961d-386ee0107fbb",
   "metadata": {},
   "source": [
    "# Ensemble Methods - Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1f7077f-ff5d-4c37-8f8a-fd8f29f14d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Stacking Classifier...\n",
      "accuracy:  0.8020833333333334\n",
      "macro avg:  {'precision': 0.8228964018437702, 'recall': 0.7917147667147668, 'f1-score': 0.7765171269007586, 'support': 192.0}\n",
      "weighted avg:  {'precision': 0.8488227780990938, 'recall': 0.8020833333333334, 'f1-score': 0.790623402835679, 'support': 192.0}\n",
      "Best Score for Stacking Classifier:  0.8020833333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Stacking Classifier...\")\n",
    "estimators = [\n",
    "    ('logreg', LogisticRegression(class_weight='balanced', C=grid_logreg.best_params_['C'], max_iter=2000, solver='liblinear', tol=1e-3)),\n",
    "    ('svm', SVC(class_weight='balanced', C=grid_svm.best_params_['C'], kernel=grid_svm.best_params_['kernel'])),\n",
    "    ('rf', RandomForestClassifier(class_weight='balanced', n_estimators=grid_rf.best_params_['n_estimators'], max_depth=grid_rf.best_params_['max_depth']))\n",
    "]\n",
    "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=RandomForestClassifier())\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "y_pred_stacking = stacking_clf.predict(X_test)\n",
    "report_stacking = classification_report(y_test, y_pred_stacking, output_dict=True, zero_division=0)\n",
    "print_classification_report(report_stacking)\n",
    "print(\"Best Score for Stacking Classifier: \", stacking_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05ce110-64f3-4871-9278-847649e6b625",
   "metadata": {},
   "source": [
    "Leveraged the strengths of multiple models, providing robust performance and demonstrating the power of ensemble methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be6fa37-36a2-4aa0-93d8-c2f747e4a827",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "The machine learning models tested illustrated that SVM and KNN were strong in terms of accuracy. Nevertheless, the Stacking Classifier amalgamated multiple models in a complementary but equally powerful way. Quality improvement for data, feature improvement, and advanced ensemble techniques are recommended for future studies that could build on the findings of this task. Upon examining various machine learning models, it was revealed that SVM and KNN excelled in terms of accuracy. Nonetheless, the Stacking Classifier managed to intermix several models thus allowing for a stronger and more balanced model at once. In order to improve the model further, I need to focus on data quality enhancement, feature engineering as well as exploring more advanced ensemble techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "45fa2903-e47a-4169-b7c3-537913c8bb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the best model and vectorizer...\n",
      "Model and vectorizer saved successfully.\n",
      "Training completed and model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Saving the Best Model and Vectorizer\n",
    "best_model = None\n",
    "best_score = 0\n",
    "for grid in [grid_logreg, grid_svm, grid_rf, grid_dt, grid_nb, grid_knn, stacking_clf]:\n",
    "    if hasattr(grid, 'best_score_') and grid.best_score_ > best_score:\n",
    "        best_model = grid.best_estimator_\n",
    "        best_score = grid.best_score_\n",
    "    elif hasattr(grid, 'score') and grid.score(X_test, y_test) > best_score:\n",
    "        best_model = grid\n",
    "        best_score = grid.score(X_test, y_test)\n",
    "\n",
    "print(\"Saving the best model and vectorizer...\")\n",
    "pickle.dump(best_model, open('model/best_model.pkl', 'wb'))\n",
    "pickle.dump(vectorizer, open('model/vectorizer.pkl', 'wb'))\n",
    "print(\"Model and vectorizer saved successfully.\")\n",
    "\n",
    "print(\"Training completed and model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed56b56-7834-4217-84fc-ab75a4ac50bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
